{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5351e52ec915480b839d8750c2740c3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8e19c01cda64e028fbc6afb96cf47ef",
              "IPY_MODEL_0f8c75d77ead42bb90e612ab81346198",
              "IPY_MODEL_fb9d2b3346ee4482bdf968fe05e7bc78"
            ],
            "layout": "IPY_MODEL_ce71525cb4174980b2382f009d3b8f3c"
          }
        },
        "c8e19c01cda64e028fbc6afb96cf47ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e09a17acb6c4de2916f7eb477bb02e0",
            "placeholder": "​",
            "style": "IPY_MODEL_9bf53df770a946fa80177bd574bb1f12",
            "value": "Loading weights: 100%"
          }
        },
        "0f8c75d77ead42bb90e612ab81346198": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_489f45ae74bc440dae29332268cb4846",
            "max": 148,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14bea93b75964d81a2683fa0cba4cfcc",
            "value": 148
          }
        },
        "fb9d2b3346ee4482bdf968fe05e7bc78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5919e57c60d449fc93aa5b577c9c8037",
            "placeholder": "​",
            "style": "IPY_MODEL_539d3376cfa741e2ad68ac4eb46e0872",
            "value": " 148/148 [00:00&lt;00:00, 381.38it/s, Materializing param=transformer.wte.weight]"
          }
        },
        "ce71525cb4174980b2382f009d3b8f3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e09a17acb6c4de2916f7eb477bb02e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bf53df770a946fa80177bd574bb1f12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "489f45ae74bc440dae29332268cb4846": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14bea93b75964d81a2683fa0cba4cfcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5919e57c60d449fc93aa5b577c9c8037": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "539d3376cfa741e2ad68ac4eb46e0872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a7cf818231d4760a416960617a722be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68fdf88eaef34620981bf12098122a38",
              "IPY_MODEL_02a9d98dc3de435cb8ee75d2e99cddac",
              "IPY_MODEL_32a2cf6d1bbd474fbfec3ec784903b9e"
            ],
            "layout": "IPY_MODEL_bde87639d31944f7aeb3163e799830c6"
          }
        },
        "68fdf88eaef34620981bf12098122a38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d818fa490a04892a10d95235c42d127",
            "placeholder": "​",
            "style": "IPY_MODEL_e9cb5789a0db402cbc97a2c91d6e2e32",
            "value": "Loading weights: 100%"
          }
        },
        "02a9d98dc3de435cb8ee75d2e99cddac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef72251026824aa9bd22b373286e827c",
            "max": 148,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04df395cb5e944ccbec0a16e322d68b1",
            "value": 148
          }
        },
        "32a2cf6d1bbd474fbfec3ec784903b9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c083c6741a454e919cb841d05f74f763",
            "placeholder": "​",
            "style": "IPY_MODEL_b28ae435300641638632303050a34295",
            "value": " 148/148 [00:00&lt;00:00, 540.50it/s, Materializing param=transformer.wte.weight]"
          }
        },
        "bde87639d31944f7aeb3163e799830c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d818fa490a04892a10d95235c42d127": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9cb5789a0db402cbc97a2c91d6e2e32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef72251026824aa9bd22b373286e827c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04df395cb5e944ccbec0a16e322d68b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c083c6741a454e919cb841d05f74f763": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b28ae435300641638632303050a34295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30b02857e768405fa7804f1e60dfb792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c6e34d6898f43f89e6e94b1cf7d28d3",
              "IPY_MODEL_d684b3568f4244e8b8a54a08a094e71c",
              "IPY_MODEL_74aa0107822743e7bbf1f47b8c62a38d"
            ],
            "layout": "IPY_MODEL_d0fff3eee8d943788f0a160574f72fdf"
          }
        },
        "9c6e34d6898f43f89e6e94b1cf7d28d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78ebe790756b433d8dbf44b26fd4706d",
            "placeholder": "​",
            "style": "IPY_MODEL_90ce0b02059541938375d948fd138d85",
            "value": "Loading weights: 100%"
          }
        },
        "d684b3568f4244e8b8a54a08a094e71c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e88f5389a9c94b0f9a029ae8a26fdb78",
            "max": 148,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75f2fee0f19741fe97110659c31b1d9e",
            "value": 148
          }
        },
        "74aa0107822743e7bbf1f47b8c62a38d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_793fe369c83d4475af29c748f5f5714a",
            "placeholder": "​",
            "style": "IPY_MODEL_69d8f80719b94b08994da83e4709ec41",
            "value": " 148/148 [00:00&lt;00:00, 538.40it/s, Materializing param=transformer.wte.weight]"
          }
        },
        "d0fff3eee8d943788f0a160574f72fdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78ebe790756b433d8dbf44b26fd4706d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90ce0b02059541938375d948fd138d85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e88f5389a9c94b0f9a029ae8a26fdb78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75f2fee0f19741fe97110659c31b1d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "793fe369c83d4475af29c748f5f5714a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69d8f80719b94b08994da83e4709ec41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Generative AI and what are its primary use cases across industries?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Generative AI refers to a class of artificial intelligence models that can generate new content such as text, images, audio, video, or code by learning patterns from existing data. Unlike traditional AI systems that classify or predict outcomes, generative models create new data samples that resemble the training data.\n",
        "\n",
        "Generative AI typically uses deep learning architectures such as Transformers, Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and diffusion models.\n",
        "\n",
        "Primary use cases across industries include:\n",
        "\n",
        "Healthcare – Drug discovery, medical report generation, and synthetic data generation.\n",
        "\n",
        "Finance – Fraud detection simulation, automated report writing, and algorithmic trading strategies.\n",
        "\n",
        "Media & Entertainment – Script writing, music generation, video editing, and game design.\n",
        "\n",
        "Education – Personalized tutoring systems and automated content creation.\n",
        "\n",
        "E-commerce – Product description generation and chatbot-based customer support.\n",
        "\n",
        "Manufacturing – Design optimization and digital twin simulations.\n",
        "\n",
        "Generative AI enhances creativity, automation, and personalization at scale.\n",
        "\n",
        "Question 2: Explain the role of probabilistic modeling in generative models. How do these models differ from discriminative models?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Probabilistic modeling forms the foundation of generative models. These models learn the joint probability distribution:\n",
        "\n",
        "P(X, Y)\n",
        "\n",
        "or simply:\n",
        "\n",
        "P(X)\n",
        "\n",
        "This allows them to generate new samples by sampling from the learned distribution.\n",
        "\n",
        "Generative models aim to model how data is generated. Examples include:\n",
        "\n",
        "Naive Bayes\n",
        "\n",
        "Hidden Markov Models\n",
        "\n",
        "Variational Autoencoders\n",
        "\n",
        "GANs\n",
        "\n",
        "Discriminative models, on the other hand, learn:\n",
        "\n",
        "P(Y | X)\n",
        "\n",
        "They focus on decision boundaries between classes rather than modeling the data distribution.\n",
        "\n",
        "Key Differences:\n",
        "\n",
        "Generative Models\tDiscriminative Models\n",
        "Learn data distribution\tLearn decision boundary\n",
        "Can generate new samples\tCannot generate new data\n",
        "More complex training\tUsually simpler training\n",
        "Example: VAE, GAN\tExample: Logistic Regression, SVM\n",
        "\n",
        "Generative models are powerful for simulation, synthesis, and creative AI applications.\n",
        "\n",
        "Question 3: What is the difference between Autoencoders and Variational Autoencoders (VAEs) in the context of text generation?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Autoencoders are neural networks designed to reconstruct input data. They consist of:\n",
        "\n",
        "Encoder: Compresses input into latent representation.\n",
        "\n",
        "Decoder: Reconstructs input from latent representation.\n",
        "\n",
        "They learn deterministic mappings.\n",
        "\n",
        "Variational Autoencoders (VAEs) extend this idea using probabilistic latent variables. Instead of learning a fixed latent vector, VAEs learn:\n",
        "\n",
        "Mean (μ) and Variance (σ²)\n",
        "\n",
        "They sample from a Gaussian distribution:\n",
        "\n",
        "z ~ N(μ, σ²)\n",
        "\n",
        "Key Differences:\n",
        "\n",
        "Autoencoder\tVariational Autoencoder\n",
        "Deterministic latent space\tProbabilistic latent space\n",
        "No sampling\tUses reparameterization trick\n",
        "Not ideal for generation\tGood for generation\n",
        "No KL divergence\tIncludes KL divergence loss\n",
        "\n",
        "In text generation, VAEs can generate diverse sentences by sampling different latent vectors.\n",
        "\n",
        "Question 4: Describe the working of attention mechanisms in Neural Machine Translation (NMT). Why are they critical?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Attention mechanisms allow the decoder to focus on relevant parts of the input sequence while generating each output word.\n",
        "\n",
        "Traditional encoder-decoder models compress the entire input into a single context vector. This causes information loss for long sentences.\n",
        "\n",
        "Attention solves this by:\n",
        "\n",
        "Calculating alignment scores between decoder hidden state and encoder outputs.\n",
        "\n",
        "Applying softmax to get attention weights.\n",
        "\n",
        "Computing weighted sum of encoder outputs.\n",
        "\n",
        "Feeding this context vector to decoder.\n",
        "\n",
        "Why critical?\n",
        "\n",
        "Handles long sentences better.\n",
        "\n",
        "Improves translation accuracy.\n",
        "\n",
        "Enables parallel computation (in Transformers).\n",
        "\n",
        "Forms the backbone of modern models like Transformers.\n",
        "\n",
        "Attention enables dynamic context learning instead of fixed compression.\n",
        "\n",
        "Question 5: What ethical considerations must be addressed when using generative AI for creative content such as poetry or storytelling?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Key ethical considerations include:\n",
        "\n",
        "Copyright Issues – AI-generated text may resemble copyrighted material.\n",
        "\n",
        "Bias – Models trained on biased data may produce discriminatory content.\n",
        "\n",
        "Misinformation – Generated stories may spread false narratives.\n",
        "\n",
        "Authenticity – Unclear ownership of AI-generated content.\n",
        "\n",
        "Deepfake Risks – Misuse in deceptive storytelling.\n",
        "\n",
        "Cultural Sensitivity – Offensive or culturally inappropriate outputs.\n",
        "\n",
        "Mitigation Strategies:\n",
        "\n",
        "Bias audits\n",
        "\n",
        "Dataset filtering\n",
        "\n",
        "Human-in-the-loop moderation\n",
        "\n",
        "Transparency in AI usage\n",
        "\n",
        "Responsible AI practices are essential for safe deployment."
      ],
      "metadata": {
        "id": "97kaWSr3VUMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6. Use the following small text dataset to train a simple Variational\n",
        "Autoencoder (VAE) for text reconstruction:\n",
        "[\"The sky is blue\", \"The sun is bright\", \"The grass is green\",\n",
        "\"The night is dark\", \"The stars are shining\"]\n",
        "1. Preprocess the data (tokenize and pad the sequences).\n",
        "2. Build a basic VAE model for text reconstruction.\n",
        "3. Train the model and show how it reconstructs or generates similar sentences.\n",
        "Include your code, explanation, and sample outputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "TkbE-qNmYohj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# =====================================\n",
        "# 2. Dataset\n",
        "# =====================================\n",
        "\n",
        "sentences = [\n",
        "    \"The sky is blue\",\n",
        "    \"The sun is bright\",\n",
        "    \"The grass is green\",\n",
        "    \"The night is dark\",\n",
        "    \"The stars are shining\"\n",
        "]\n",
        "\n",
        "# =====================================\n",
        "# 3. Tokenization & Padding\n",
        "# =====================================\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "max_len = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(\"Word Index:\\n\", tokenizer.word_index)\n",
        "print(\"\\nPadded Sequences:\\n\", padded_sequences)\n",
        "print(\"\\nVocabulary Size:\", vocab_size)\n",
        "print(\"Max Sequence Length:\", max_len)\n",
        "\n",
        "# =====================================\n",
        "# 4. Build Encoder\n",
        "# =====================================\n",
        "\n",
        "embedding_dim = 16\n",
        "latent_dim = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(max_len,))\n",
        "x = layers.Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "z_mean = layers.Dense(latent_dim)(x)\n",
        "z_log_var = layers.Dense(latent_dim)(x)\n",
        "\n",
        "# Sampling layer\n",
        "class Sampling(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "# =====================================\n",
        "# 5. Build Decoder\n",
        "# =====================================\n",
        "\n",
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(max_len * vocab_size, activation=\"softmax\")(latent_inputs)\n",
        "decoder_outputs = layers.Reshape((max_len, vocab_size))(x)\n",
        "\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "\n",
        "# =====================================\n",
        "# 6. Define VAE Model (Subclassing)\n",
        "# =====================================\n",
        "\n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                keras.losses.sparse_categorical_crossentropy(data, reconstruction)\n",
        "            )\n",
        "\n",
        "            kl_loss = -0.5 * tf.reduce_mean(\n",
        "                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "            )\n",
        "\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "# =====================================\n",
        "# 7. Train VAE\n",
        "# =====================================\n",
        "\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=\"adam\")\n",
        "\n",
        "vae.fit(padded_sequences, epochs=200, batch_size=2)\n",
        "\n",
        "# =====================================\n",
        "# 8. Reconstruction\n",
        "# =====================================\n",
        "\n",
        "z_mean, z_log_var, z = encoder.predict(padded_sequences)\n",
        "reconstructed = decoder.predict(z)\n",
        "\n",
        "print(\"\\n===== Reconstruction Results =====\\n\")\n",
        "\n",
        "for i in range(len(reconstructed)):\n",
        "    predicted_sequence = np.argmax(reconstructed[i], axis=1)\n",
        "    words = [tokenizer.index_word.get(idx, '') for idx in predicted_sequence]\n",
        "\n",
        "    print(\"Original:     \", sentences[i])\n",
        "    print(\"Reconstructed:\", \" \".join(words))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S3FiqhZZa8g",
        "outputId": "ca214cd1-246c-49b6-e623-0b6d8c91d8c6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Index:\n",
            " {'the': 1, 'is': 2, 'sky': 3, 'blue': 4, 'sun': 5, 'bright': 6, 'grass': 7, 'green': 8, 'night': 9, 'dark': 10, 'stars': 11, 'are': 12, 'shining': 13}\n",
            "\n",
            "Padded Sequences:\n",
            " [[ 1  3  2  4]\n",
            " [ 1  5  2  6]\n",
            " [ 1  7  2  8]\n",
            " [ 1  9  2 10]\n",
            " [ 1 11 12 13]]\n",
            "\n",
            "Vocabulary Size: 14\n",
            "Max Sequence Length: 4\n",
            "Epoch 1/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - kl_loss: 0.0012 - loss: 2.7779 - reconstruction_loss: 2.7768\n",
            "Epoch 2/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - kl_loss: 0.0011 - loss: 2.6840 - reconstruction_loss: 2.6829\n",
            "Epoch 3/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - kl_loss: 0.0011 - loss: 2.7599 - reconstruction_loss: 2.7588\n",
            "Epoch 4/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - kl_loss: 0.0011 - loss: 2.6016 - reconstruction_loss: 2.6005    \n",
            "Epoch 5/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - kl_loss: 0.0013 - loss: 2.7325 - reconstruction_loss: 2.7312\n",
            "Epoch 6/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - kl_loss: 0.0016 - loss: 2.7490 - reconstruction_loss: 2.7474\n",
            "Epoch 7/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - kl_loss: 0.0019 - loss: 2.7278 - reconstruction_loss: 2.7259 \n",
            "Epoch 8/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - kl_loss: 0.0023 - loss: 2.6159 - reconstruction_loss: 2.6137\n",
            "Epoch 9/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - kl_loss: 0.0029 - loss: 2.5715 - reconstruction_loss: 2.5686\n",
            "Epoch 10/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - kl_loss: 0.0032 - loss: 2.6111 - reconstruction_loss: 2.6078\n",
            "Epoch 11/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - kl_loss: 0.0040 - loss: 2.7118 - reconstruction_loss: 2.7078\n",
            "Epoch 12/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - kl_loss: 0.0048 - loss: 2.6202 - reconstruction_loss: 2.6153\n",
            "Epoch 13/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - kl_loss: 0.0056 - loss: 2.5300 - reconstruction_loss: 2.5244\n",
            "Epoch 14/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - kl_loss: 0.0066 - loss: 2.4670 - reconstruction_loss: 2.4604\n",
            "Epoch 15/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - kl_loss: 0.0076 - loss: 2.5596 - reconstruction_loss: 2.5519\n",
            "Epoch 16/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - kl_loss: 0.0086 - loss: 2.6070 - reconstruction_loss: 2.5984\n",
            "Epoch 17/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - kl_loss: 0.0096 - loss: 2.6895 - reconstruction_loss: 2.6799\n",
            "Epoch 18/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - kl_loss: 0.0110 - loss: 2.8440 - reconstruction_loss: 2.8330\n",
            "Epoch 19/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - kl_loss: 0.0130 - loss: 2.7124 - reconstruction_loss: 2.6994\n",
            "Epoch 20/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - kl_loss: 0.0144 - loss: 2.5649 - reconstruction_loss: 2.5505\n",
            "Epoch 21/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - kl_loss: 0.0159 - loss: 2.4704 - reconstruction_loss: 2.4544\n",
            "Epoch 22/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - kl_loss: 0.0188 - loss: 2.4935 - reconstruction_loss: 2.4748\n",
            "Epoch 23/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - kl_loss: 0.0210 - loss: 2.6081 - reconstruction_loss: 2.5871\n",
            "Epoch 24/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - kl_loss: 0.0230 - loss: 2.4275 - reconstruction_loss: 2.4045\n",
            "Epoch 25/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - kl_loss: 0.0247 - loss: 2.5242 - reconstruction_loss: 2.4995\n",
            "Epoch 26/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - kl_loss: 0.0283 - loss: 2.5357 - reconstruction_loss: 2.5073\n",
            "Epoch 27/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - kl_loss: 0.0320 - loss: 2.4449 - reconstruction_loss: 2.4129\n",
            "Epoch 28/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - kl_loss: 0.0345 - loss: 2.5717 - reconstruction_loss: 2.5371\n",
            "Epoch 29/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - kl_loss: 0.0375 - loss: 2.3530 - reconstruction_loss: 2.3156\n",
            "Epoch 30/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - kl_loss: 0.0409 - loss: 2.7548 - reconstruction_loss: 2.7139\n",
            "Epoch 31/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - kl_loss: 0.0448 - loss: 2.3690 - reconstruction_loss: 2.3242\n",
            "Epoch 32/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - kl_loss: 0.0479 - loss: 2.3200 - reconstruction_loss: 2.2721\n",
            "Epoch 33/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - kl_loss: 0.0545 - loss: 2.5447 - reconstruction_loss: 2.4902\n",
            "Epoch 34/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - kl_loss: 0.0569 - loss: 2.4274 - reconstruction_loss: 2.3705\n",
            "Epoch 35/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - kl_loss: 0.0613 - loss: 2.4672 - reconstruction_loss: 2.4059\n",
            "Epoch 36/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - kl_loss: 0.0665 - loss: 2.5620 - reconstruction_loss: 2.4955 \n",
            "Epoch 37/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - kl_loss: 0.0688 - loss: 2.4544 - reconstruction_loss: 2.3856\n",
            "Epoch 38/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - kl_loss: 0.0780 - loss: 2.7285 - reconstruction_loss: 2.6505\n",
            "Epoch 39/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - kl_loss: 0.0802 - loss: 2.2648 - reconstruction_loss: 2.1846 \n",
            "Epoch 40/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - kl_loss: 0.0831 - loss: 2.1041 - reconstruction_loss: 2.0209\n",
            "Epoch 41/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - kl_loss: 0.0927 - loss: 2.3377 - reconstruction_loss: 2.2450\n",
            "Epoch 42/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - kl_loss: 0.1016 - loss: 2.3122 - reconstruction_loss: 2.2106\n",
            "Epoch 43/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - kl_loss: 0.1050 - loss: 2.4409 - reconstruction_loss: 2.3359\n",
            "Epoch 44/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - kl_loss: 0.1133 - loss: 2.4417 - reconstruction_loss: 2.3285\n",
            "Epoch 45/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - kl_loss: 0.1112 - loss: 2.3626 - reconstruction_loss: 2.2514 \n",
            "Epoch 46/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - kl_loss: 0.1223 - loss: 2.3425 - reconstruction_loss: 2.2202\n",
            "Epoch 47/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - kl_loss: 0.1245 - loss: 2.3348 - reconstruction_loss: 2.2103\n",
            "Epoch 48/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - kl_loss: 0.1405 - loss: 2.1810 - reconstruction_loss: 2.0405\n",
            "Epoch 49/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - kl_loss: 0.1452 - loss: 2.2610 - reconstruction_loss: 2.1158\n",
            "Epoch 50/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - kl_loss: 0.1482 - loss: 2.3068 - reconstruction_loss: 2.1586\n",
            "Epoch 51/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - kl_loss: 0.1571 - loss: 2.3775 - reconstruction_loss: 2.2204\n",
            "Epoch 52/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - kl_loss: 0.1678 - loss: 2.1138 - reconstruction_loss: 1.9460 \n",
            "Epoch 53/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - kl_loss: 0.1683 - loss: 1.9684 - reconstruction_loss: 1.8002\n",
            "Epoch 54/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - kl_loss: 0.1809 - loss: 2.0853 - reconstruction_loss: 1.9044 \n",
            "Epoch 55/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - kl_loss: 0.1713 - loss: 2.3311 - reconstruction_loss: 2.1598 \n",
            "Epoch 56/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - kl_loss: 0.1865 - loss: 2.3626 - reconstruction_loss: 2.1761\n",
            "Epoch 57/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - kl_loss: 0.1944 - loss: 2.0770 - reconstruction_loss: 1.8826\n",
            "Epoch 58/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - kl_loss: 0.2054 - loss: 2.1471 - reconstruction_loss: 1.9418\n",
            "Epoch 59/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - kl_loss: 0.2045 - loss: 2.1846 - reconstruction_loss: 1.9801\n",
            "Epoch 60/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - kl_loss: 0.2166 - loss: 2.1132 - reconstruction_loss: 1.8966\n",
            "Epoch 61/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - kl_loss: 0.2143 - loss: 1.9118 - reconstruction_loss: 1.6976\n",
            "Epoch 62/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - kl_loss: 0.2263 - loss: 2.1142 - reconstruction_loss: 1.8879 \n",
            "Epoch 63/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - kl_loss: 0.2241 - loss: 2.0843 - reconstruction_loss: 1.8602\n",
            "Epoch 64/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - kl_loss: 0.2354 - loss: 2.0756 - reconstruction_loss: 1.8402\n",
            "Epoch 65/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - kl_loss: 0.2391 - loss: 1.9581 - reconstruction_loss: 1.7189\n",
            "Epoch 66/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - kl_loss: 0.2532 - loss: 2.1935 - reconstruction_loss: 1.9403\n",
            "Epoch 67/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - kl_loss: 0.2451 - loss: 2.0311 - reconstruction_loss: 1.7860 \n",
            "Epoch 68/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - kl_loss: 0.2697 - loss: 2.1175 - reconstruction_loss: 1.8477\n",
            "Epoch 69/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - kl_loss: 0.2771 - loss: 2.1709 - reconstruction_loss: 1.8938\n",
            "Epoch 70/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - kl_loss: 0.2847 - loss: 2.2250 - reconstruction_loss: 1.9403 \n",
            "Epoch 71/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - kl_loss: 0.2920 - loss: 2.0285 - reconstruction_loss: 1.7366\n",
            "Epoch 72/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - kl_loss: 0.3010 - loss: 2.0645 - reconstruction_loss: 1.7635\n",
            "Epoch 73/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - kl_loss: 0.3060 - loss: 2.1148 - reconstruction_loss: 1.8088\n",
            "Epoch 74/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - kl_loss: 0.3139 - loss: 2.0230 - reconstruction_loss: 1.7091 \n",
            "Epoch 75/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - kl_loss: 0.3126 - loss: 1.9568 - reconstruction_loss: 1.6443\n",
            "Epoch 76/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - kl_loss: 0.3136 - loss: 1.8701 - reconstruction_loss: 1.5565\n",
            "Epoch 77/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - kl_loss: 0.3096 - loss: 1.8896 - reconstruction_loss: 1.5799\n",
            "Epoch 78/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - kl_loss: 0.3168 - loss: 1.9299 - reconstruction_loss: 1.6131\n",
            "Epoch 79/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - kl_loss: 0.3037 - loss: 1.7401 - reconstruction_loss: 1.4364\n",
            "Epoch 80/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - kl_loss: 0.3292 - loss: 1.8820 - reconstruction_loss: 1.5528\n",
            "Epoch 81/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - kl_loss: 0.3234 - loss: 1.8964 - reconstruction_loss: 1.5730\n",
            "Epoch 82/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - kl_loss: 0.3291 - loss: 2.0404 - reconstruction_loss: 1.7113\n",
            "Epoch 83/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - kl_loss: 0.3321 - loss: 1.7567 - reconstruction_loss: 1.4247\n",
            "Epoch 84/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - kl_loss: 0.3319 - loss: 2.1112 - reconstruction_loss: 1.7793 \n",
            "Epoch 85/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - kl_loss: 0.3364 - loss: 2.1944 - reconstruction_loss: 1.8580\n",
            "Epoch 86/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - kl_loss: 0.3516 - loss: 1.7868 - reconstruction_loss: 1.4351\n",
            "Epoch 87/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - kl_loss: 0.3425 - loss: 1.7641 - reconstruction_loss: 1.4216\n",
            "Epoch 88/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - kl_loss: 0.3523 - loss: 1.9107 - reconstruction_loss: 1.5584\n",
            "Epoch 89/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - kl_loss: 0.3504 - loss: 1.5419 - reconstruction_loss: 1.1915\n",
            "Epoch 90/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - kl_loss: 0.3333 - loss: 1.9848 - reconstruction_loss: 1.6515\n",
            "Epoch 91/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - kl_loss: 0.3561 - loss: 1.8730 - reconstruction_loss: 1.5169\n",
            "Epoch 92/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - kl_loss: 0.3273 - loss: 1.8380 - reconstruction_loss: 1.5108\n",
            "Epoch 93/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - kl_loss: 0.3312 - loss: 1.8108 - reconstruction_loss: 1.4796\n",
            "Epoch 94/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - kl_loss: 0.3509 - loss: 1.8435 - reconstruction_loss: 1.4926\n",
            "Epoch 95/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - kl_loss: 0.3516 - loss: 1.7586 - reconstruction_loss: 1.4069\n",
            "Epoch 96/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - kl_loss: 0.3426 - loss: 1.8382 - reconstruction_loss: 1.4956 \n",
            "Epoch 97/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - kl_loss: 0.3431 - loss: 1.8403 - reconstruction_loss: 1.4973 \n",
            "Epoch 98/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - kl_loss: 0.3417 - loss: 1.8197 - reconstruction_loss: 1.4780\n",
            "Epoch 99/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - kl_loss: 0.3426 - loss: 1.6600 - reconstruction_loss: 1.3174\n",
            "Epoch 100/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - kl_loss: 0.3610 - loss: 1.6313 - reconstruction_loss: 1.2703 \n",
            "Epoch 101/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - kl_loss: 0.3453 - loss: 1.8715 - reconstruction_loss: 1.5262 \n",
            "Epoch 102/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - kl_loss: 0.3454 - loss: 1.8411 - reconstruction_loss: 1.4957\n",
            "Epoch 103/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - kl_loss: 0.3511 - loss: 1.7819 - reconstruction_loss: 1.4308 \n",
            "Epoch 104/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - kl_loss: 0.3627 - loss: 1.5135 - reconstruction_loss: 1.1508 \n",
            "Epoch 105/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - kl_loss: 0.3617 - loss: 1.6552 - reconstruction_loss: 1.2936 \n",
            "Epoch 106/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - kl_loss: 0.3312 - loss: 1.6139 - reconstruction_loss: 1.2827\n",
            "Epoch 107/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - kl_loss: 0.3534 - loss: 1.7125 - reconstruction_loss: 1.3590 \n",
            "Epoch 108/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - kl_loss: 0.3746 - loss: 1.7958 - reconstruction_loss: 1.4212 \n",
            "Epoch 109/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - kl_loss: 0.3481 - loss: 1.6838 - reconstruction_loss: 1.3358 \n",
            "Epoch 110/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - kl_loss: 0.3883 - loss: 1.7462 - reconstruction_loss: 1.3579 \n",
            "Epoch 111/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - kl_loss: 0.3768 - loss: 1.5613 - reconstruction_loss: 1.1845\n",
            "Epoch 112/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - kl_loss: 0.3816 - loss: 1.5328 - reconstruction_loss: 1.1512\n",
            "Epoch 113/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - kl_loss: 0.3904 - loss: 1.6846 - reconstruction_loss: 1.2942\n",
            "Epoch 114/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - kl_loss: 0.3903 - loss: 1.8201 - reconstruction_loss: 1.4298\n",
            "Epoch 115/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - kl_loss: 0.3836 - loss: 1.7032 - reconstruction_loss: 1.3196\n",
            "Epoch 116/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - kl_loss: 0.4021 - loss: 1.5995 - reconstruction_loss: 1.1973\n",
            "Epoch 117/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - kl_loss: 0.3868 - loss: 1.4565 - reconstruction_loss: 1.0697\n",
            "Epoch 118/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - kl_loss: 0.3893 - loss: 1.6821 - reconstruction_loss: 1.2927\n",
            "Epoch 119/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - kl_loss: 0.3901 - loss: 1.4995 - reconstruction_loss: 1.1093\n",
            "Epoch 120/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - kl_loss: 0.3917 - loss: 1.6492 - reconstruction_loss: 1.2574\n",
            "Epoch 121/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - kl_loss: 0.3617 - loss: 1.6264 - reconstruction_loss: 1.2648\n",
            "Epoch 122/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - kl_loss: 0.3925 - loss: 1.5736 - reconstruction_loss: 1.1811\n",
            "Epoch 123/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - kl_loss: 0.3833 - loss: 1.5238 - reconstruction_loss: 1.1405\n",
            "Epoch 124/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - kl_loss: 0.3790 - loss: 1.5013 - reconstruction_loss: 1.1223\n",
            "Epoch 125/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - kl_loss: 0.3806 - loss: 1.6529 - reconstruction_loss: 1.2723\n",
            "Epoch 126/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - kl_loss: 0.4000 - loss: 1.7589 - reconstruction_loss: 1.3589\n",
            "Epoch 127/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - kl_loss: 0.4002 - loss: 1.8980 - reconstruction_loss: 1.4978\n",
            "Epoch 128/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - kl_loss: 0.4188 - loss: 1.7703 - reconstruction_loss: 1.3515\n",
            "Epoch 129/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - kl_loss: 0.4370 - loss: 1.5203 - reconstruction_loss: 1.0833\n",
            "Epoch 130/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - kl_loss: 0.4245 - loss: 1.4992 - reconstruction_loss: 1.0747\n",
            "Epoch 131/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - kl_loss: 0.4467 - loss: 1.7220 - reconstruction_loss: 1.2753\n",
            "Epoch 132/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - kl_loss: 0.4459 - loss: 1.6654 - reconstruction_loss: 1.2195\n",
            "Epoch 133/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - kl_loss: 0.4490 - loss: 1.5173 - reconstruction_loss: 1.0683 \n",
            "Epoch 134/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - kl_loss: 0.4432 - loss: 1.5507 - reconstruction_loss: 1.1075\n",
            "Epoch 135/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - kl_loss: 0.4325 - loss: 1.4233 - reconstruction_loss: 0.9908\n",
            "Epoch 136/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - kl_loss: 0.4156 - loss: 1.3068 - reconstruction_loss: 0.8912\n",
            "Epoch 137/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - kl_loss: 0.3738 - loss: 1.5090 - reconstruction_loss: 1.1353 \n",
            "Epoch 138/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - kl_loss: 0.3696 - loss: 1.6626 - reconstruction_loss: 1.2929\n",
            "Epoch 139/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - kl_loss: 0.3618 - loss: 1.6530 - reconstruction_loss: 1.2912\n",
            "Epoch 140/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - kl_loss: 0.3497 - loss: 1.3775 - reconstruction_loss: 1.0278 \n",
            "Epoch 141/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - kl_loss: 0.3660 - loss: 1.5780 - reconstruction_loss: 1.2120\n",
            "Epoch 142/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - kl_loss: 0.3841 - loss: 1.5724 - reconstruction_loss: 1.1883\n",
            "Epoch 143/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - kl_loss: 0.3857 - loss: 1.2513 - reconstruction_loss: 0.8656\n",
            "Epoch 144/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - kl_loss: 0.3805 - loss: 1.4703 - reconstruction_loss: 1.0897\n",
            "Epoch 145/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - kl_loss: 0.3739 - loss: 1.7839 - reconstruction_loss: 1.4100\n",
            "Epoch 146/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - kl_loss: 0.3974 - loss: 1.4556 - reconstruction_loss: 1.0582\n",
            "Epoch 147/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - kl_loss: 0.3940 - loss: 1.3790 - reconstruction_loss: 0.9850\n",
            "Epoch 148/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - kl_loss: 0.3858 - loss: 1.3178 - reconstruction_loss: 0.9321\n",
            "Epoch 149/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - kl_loss: 0.3885 - loss: 1.4617 - reconstruction_loss: 1.0732\n",
            "Epoch 150/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - kl_loss: 0.3870 - loss: 1.4799 - reconstruction_loss: 1.0930\n",
            "Epoch 151/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - kl_loss: 0.3834 - loss: 1.3525 - reconstruction_loss: 0.9691\n",
            "Epoch 152/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - kl_loss: 0.3879 - loss: 1.2993 - reconstruction_loss: 0.9114\n",
            "Epoch 153/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - kl_loss: 0.3781 - loss: 1.2056 - reconstruction_loss: 0.8275\n",
            "Epoch 154/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - kl_loss: 0.3571 - loss: 1.5777 - reconstruction_loss: 1.2205\n",
            "Epoch 155/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - kl_loss: 0.3417 - loss: 1.2096 - reconstruction_loss: 0.8679\n",
            "Epoch 156/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - kl_loss: 0.3337 - loss: 1.3131 - reconstruction_loss: 0.9794\n",
            "Epoch 157/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - kl_loss: 0.3234 - loss: 1.4477 - reconstruction_loss: 1.1244\n",
            "Epoch 158/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - kl_loss: 0.3282 - loss: 1.4733 - reconstruction_loss: 1.1451\n",
            "Epoch 159/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - kl_loss: 0.3383 - loss: 1.3128 - reconstruction_loss: 0.9745\n",
            "Epoch 160/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - kl_loss: 0.3386 - loss: 1.6626 - reconstruction_loss: 1.3240\n",
            "Epoch 161/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - kl_loss: 0.3528 - loss: 1.4055 - reconstruction_loss: 1.0527\n",
            "Epoch 162/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - kl_loss: 0.3439 - loss: 1.4551 - reconstruction_loss: 1.1112\n",
            "Epoch 163/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - kl_loss: 0.3705 - loss: 1.2582 - reconstruction_loss: 0.8877\n",
            "Epoch 164/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - kl_loss: 0.3540 - loss: 1.3123 - reconstruction_loss: 0.9582\n",
            "Epoch 165/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - kl_loss: 0.3701 - loss: 1.2631 - reconstruction_loss: 0.8931\n",
            "Epoch 166/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - kl_loss: 0.3841 - loss: 1.5320 - reconstruction_loss: 1.1479\n",
            "Epoch 167/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - kl_loss: 0.3696 - loss: 1.3804 - reconstruction_loss: 1.0108\n",
            "Epoch 168/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - kl_loss: 0.3899 - loss: 1.4269 - reconstruction_loss: 1.0370\n",
            "Epoch 169/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - kl_loss: 0.3847 - loss: 1.3702 - reconstruction_loss: 0.9856\n",
            "Epoch 170/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - kl_loss: 0.3916 - loss: 1.2635 - reconstruction_loss: 0.8719\n",
            "Epoch 171/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - kl_loss: 0.3997 - loss: 1.4817 - reconstruction_loss: 1.0820\n",
            "Epoch 172/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - kl_loss: 0.4065 - loss: 1.2383 - reconstruction_loss: 0.8318\n",
            "Epoch 173/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - kl_loss: 0.4054 - loss: 1.2678 - reconstruction_loss: 0.8624\n",
            "Epoch 174/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - kl_loss: 0.3964 - loss: 1.1745 - reconstruction_loss: 0.7781\n",
            "Epoch 175/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - kl_loss: 0.3996 - loss: 1.3935 - reconstruction_loss: 0.9939\n",
            "Epoch 176/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - kl_loss: 0.3853 - loss: 1.3967 - reconstruction_loss: 1.0114 \n",
            "Epoch 177/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - kl_loss: 0.3947 - loss: 1.5108 - reconstruction_loss: 1.1160\n",
            "Epoch 178/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - kl_loss: 0.3941 - loss: 1.4365 - reconstruction_loss: 1.0424\n",
            "Epoch 179/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - kl_loss: 0.4006 - loss: 1.2730 - reconstruction_loss: 0.8723\n",
            "Epoch 180/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - kl_loss: 0.3990 - loss: 1.5735 - reconstruction_loss: 1.1745\n",
            "Epoch 181/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - kl_loss: 0.4105 - loss: 1.2835 - reconstruction_loss: 0.8731\n",
            "Epoch 182/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - kl_loss: 0.4072 - loss: 1.2500 - reconstruction_loss: 0.8428\n",
            "Epoch 183/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - kl_loss: 0.4098 - loss: 1.5250 - reconstruction_loss: 1.1152\n",
            "Epoch 184/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - kl_loss: 0.4093 - loss: 1.1720 - reconstruction_loss: 0.7627\n",
            "Epoch 185/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - kl_loss: 0.3991 - loss: 1.4238 - reconstruction_loss: 1.0247\n",
            "Epoch 186/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - kl_loss: 0.4128 - loss: 1.2032 - reconstruction_loss: 0.7904\n",
            "Epoch 187/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - kl_loss: 0.3962 - loss: 1.3996 - reconstruction_loss: 1.0034\n",
            "Epoch 188/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - kl_loss: 0.4032 - loss: 1.6442 - reconstruction_loss: 1.2410\n",
            "Epoch 189/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - kl_loss: 0.4024 - loss: 1.5193 - reconstruction_loss: 1.1168\n",
            "Epoch 190/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - kl_loss: 0.4072 - loss: 1.4006 - reconstruction_loss: 0.9934\n",
            "Epoch 191/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - kl_loss: 0.4121 - loss: 1.3029 - reconstruction_loss: 0.8908\n",
            "Epoch 192/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - kl_loss: 0.4109 - loss: 1.1880 - reconstruction_loss: 0.7770\n",
            "Epoch 193/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - kl_loss: 0.4121 - loss: 1.4048 - reconstruction_loss: 0.9927\n",
            "Epoch 194/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - kl_loss: 0.4059 - loss: 1.7117 - reconstruction_loss: 1.3058\n",
            "Epoch 195/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - kl_loss: 0.4047 - loss: 1.3258 - reconstruction_loss: 0.9211\n",
            "Epoch 196/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - kl_loss: 0.4100 - loss: 1.1410 - reconstruction_loss: 0.7310\n",
            "Epoch 197/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - kl_loss: 0.4107 - loss: 1.3894 - reconstruction_loss: 0.9787\n",
            "Epoch 198/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - kl_loss: 0.4142 - loss: 1.3281 - reconstruction_loss: 0.9139\n",
            "Epoch 199/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - kl_loss: 0.4119 - loss: 1.5412 - reconstruction_loss: 1.1294\n",
            "Epoch 200/200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - kl_loss: 0.4290 - loss: 1.2418 - reconstruction_loss: 0.8129\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\n",
            "===== Reconstruction Results =====\n",
            "\n",
            "Original:      The sky is blue\n",
            "Reconstructed: the sky is blue\n",
            "\n",
            "Original:      The sun is bright\n",
            "Reconstructed: the night is bright\n",
            "\n",
            "Original:      The grass is green\n",
            "Reconstructed: the night is blue\n",
            "\n",
            "Original:      The night is dark\n",
            "Reconstructed: the sun is dark\n",
            "\n",
            "Original:      The stars are shining\n",
            "Reconstructed: the stars is shining\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7. Use a pre-trained GPT model (like GPT-2 or GPT-3) to translate a short\n",
        "English paragraph into French and German. Provide the original and translated text.\n",
        ""
      ],
      "metadata": {
        "id": "88uL60pDao12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ============================================\n",
        "# 2. Import Libraries\n",
        "# ============================================\n",
        "\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# ============================================\n",
        "# 3. Load Pre-trained GPT-2 Model\n",
        "# ============================================\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# ============================================\n",
        "# 4. Define English Paragraph\n",
        "# ============================================\n",
        "\n",
        "english_text = \"\"\"\n",
        "Artificial Intelligence is transforming industries worldwide.\n",
        "It improves efficiency, enables automation, and creates new opportunities for innovation.\n",
        "\"\"\"\n",
        "\n",
        "print(\"========== ORIGINAL ENGLISH TEXT ==========\\n\")\n",
        "print(english_text)\n",
        "\n",
        "# ============================================\n",
        "# 5. Translate into French\n",
        "# ============================================\n",
        "\n",
        "prompt_french = f\"\"\"\n",
        "Translate the following English text into French:\n",
        "\n",
        "{english_text}\n",
        "\n",
        "French Translation:\n",
        "\"\"\"\n",
        "\n",
        "french_output = generator(prompt_french, max_length=200, num_return_sequences=1)\n",
        "\n",
        "# Extract only translated portion\n",
        "french_translation = french_output[0]['generated_text'].split(\"French Translation:\")[-1].strip()\n",
        "\n",
        "print(\"\\n========== FRENCH TRANSLATION ==========\\n\")\n",
        "print(french_translation)\n",
        "\n",
        "# ============================================\n",
        "# 6. Translate into German\n",
        "# ============================================\n",
        "\n",
        "prompt_german = f\"\"\"\n",
        "Translate the following English text into German:\n",
        "\n",
        "{english_text}\n",
        "\n",
        "German Translation:\n",
        "\"\"\"\n",
        "\n",
        "german_output = generator(prompt_german, max_length=200, num_return_sequences=1)\n",
        "\n",
        "german_translation = german_output[0]['generated_text'].split(\"German Translation:\")[-1].strip()\n",
        "\n",
        "print(\"\\n========== GERMAN TRANSLATION ==========\\n\")\n",
        "print(german_translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5351e52ec915480b839d8750c2740c3c",
            "c8e19c01cda64e028fbc6afb96cf47ef",
            "0f8c75d77ead42bb90e612ab81346198",
            "fb9d2b3346ee4482bdf968fe05e7bc78",
            "ce71525cb4174980b2382f009d3b8f3c",
            "1e09a17acb6c4de2916f7eb477bb02e0",
            "9bf53df770a946fa80177bd574bb1f12",
            "489f45ae74bc440dae29332268cb4846",
            "14bea93b75964d81a2683fa0cba4cfcc",
            "5919e57c60d449fc93aa5b577c9c8037",
            "539d3376cfa741e2ad68ac4eb46e0872"
          ]
        },
        "id": "-x7nUrELZ80m",
        "outputId": "b2adb9e8-0ffb-4ec9-f538-3bb470594959"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5351e52ec915480b839d8750c2740c3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
            "Key                  | Status     |  | \n",
            "---------------------+------------+--+-\n",
            "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== ORIGINAL ENGLISH TEXT ==========\n",
            "\n",
            "\n",
            "Artificial Intelligence is transforming industries worldwide.\n",
            "It improves efficiency, enables automation, and creates new opportunities for innovation.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== FRENCH TRANSLATION ==========\n",
            "\n",
            "\"Artificial intelligence\" is a concept that has been around for a long time. It is a term that translates to \"intelligent\" or \"intelligent\" technology that will help make us smarter, healthier, and more productive.\n",
            "\n",
            "\n",
            "The most recent example of artificial intelligence in action is the United States. It has been called the \"Jellybean of Artificial Intelligence\", and has been used in many fields such as health and medicine.\n",
            "\n",
            "Artificial Intelligence is transforming industries worldwide.Technology changes the world for the better. It is a concept that has been around for a long time. It is a term that translates to \"intelligent\" or \"intelligent\" technology that will help make us smarter, healthier, and more productive.The most recent example of artificial intelligence in action is the United States. It has been called the \"Jellybean of Artificial Intelligence\", and has been used in many fields such as health and medicine.\n",
            "\n",
            "Artificial Intelligence is transforming industries worldwide.Technology changes the world for the better. It is a concept that has been around for a long time. It is a term that translates to \"intelligent\" or \"intelligent\" technology that will help make us smarter, healthier, and more productive.The most recent example\n",
            "\n",
            "========== GERMAN TRANSLATION ==========\n",
            "\n",
            "Artificial Intelligence is transforming industries worldwide.It improves efficiency, enables automation, and creates new opportunities for innovation.\n",
            "\n",
            "\n",
            "The German word \"AI\" is a synonym of \"AI\", and \"AI\" is the Russian word \"AI\".\n",
            "\n",
            "In order to understand the differences between the two words, I have used the word \"AI\" as a synonym for \"AI\" in English. The words \"AI\" and \"AI\" are identical.\n",
            "\n",
            "\n",
            "The German word \"AI\" is a synonym of \"AI\", and \"AI\" is the Russian word \"AI\".The German word \"AI\" is a synonym of \"AI\", and \"AI\" is the Russian word \"AI\".\n",
            "\n",
            "The following words constitute the same word:\n",
            "\n",
            "\"AI\"\n",
            "\n",
            "\"AI-AI\"\n",
            "\n",
            "\"AI-AI\"\n",
            "\n",
            "\"AI\"\n",
            "\n",
            "\"AI\"\n",
            "\n",
            "\"AI\"\n",
            "\n",
            "\"AI\"\n",
            "\n",
            "\"AI\"\n",
            "\n",
            "\"AI\"\n",
            "\n",
            "\"AI\"\n",
            "\n",
            "\"AI\"\n",
            "\n",
            "\"AI\"\n",
            "\n",
            "\"AI\"\n",
            "\n",
            "\"AI\"\n",
            "\n",
            "\"AI\"\n",
            "\n",
            "\"AI\"\n",
            "\n",
            "\"AI\"\n",
            "\n",
            "\"AI\"\n",
            "\n",
            "\"AI\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8.  Implement a simple attention-based encoder-decoder model for\n",
        " # English-to-Spanish translation using Tensorflow or PyTorch.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# ============================================\n",
        "# 2. Small Toy Dataset\n",
        "# ============================================\n",
        "\n",
        "english_sentences = [\n",
        "    \"hello\",\n",
        "    \"how are you\",\n",
        "    \"i am fine\",\n",
        "    \"good morning\",\n",
        "    \"thank you\"\n",
        "]\n",
        "\n",
        "spanish_sentences = [\n",
        "    \"hola\",\n",
        "    \"como estas\",\n",
        "    \"estoy bien\",\n",
        "    \"buenos dias\",\n",
        "    \"gracias\"\n",
        "]\n",
        "\n",
        "# Add start and end tokens\n",
        "spanish_sentences = [\"<start> \" + s + \" <end>\" for s in spanish_sentences]\n",
        "\n",
        "# ============================================\n",
        "# 3. Tokenization\n",
        "# ============================================\n",
        "\n",
        "eng_tokenizer = keras.preprocessing.text.Tokenizer(filters='')\n",
        "eng_tokenizer.fit_on_texts(english_sentences)\n",
        "eng_seq = eng_tokenizer.texts_to_sequences(english_sentences)\n",
        "eng_max_len = max(len(seq) for seq in eng_seq)\n",
        "eng_seq = keras.preprocessing.sequence.pad_sequences(eng_seq, maxlen=eng_max_len, padding='post')\n",
        "\n",
        "spa_tokenizer = keras.preprocessing.text.Tokenizer(filters='')\n",
        "spa_tokenizer.fit_on_texts(spanish_sentences)\n",
        "spa_seq = spa_tokenizer.texts_to_sequences(spanish_sentences)\n",
        "spa_max_len = max(len(seq) for seq in spa_seq)\n",
        "spa_seq = keras.preprocessing.sequence.pad_sequences(spa_seq, maxlen=spa_max_len, padding='post')\n",
        "\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "spa_vocab_size = len(spa_tokenizer.word_index) + 1\n",
        "\n",
        "print(\"English Vocabulary Size:\", eng_vocab_size)\n",
        "print(\"Spanish Vocabulary Size:\", spa_vocab_size)\n",
        "\n",
        "# ============================================\n",
        "# 4. Build Encoder\n",
        "# ============================================\n",
        "\n",
        "embedding_dim = 64\n",
        "units = 128\n",
        "\n",
        "class Encoder(keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = layers.Embedding(eng_vocab_size, embedding_dim)\n",
        "        self.lstm = layers.LSTM(units, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, state_h, state_c = self.lstm(x)\n",
        "        return output, state_h, state_c\n",
        "\n",
        "# ============================================\n",
        "# 5. Bahdanau Attention\n",
        "# ============================================\n",
        "\n",
        "class BahdanauAttention(layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.W1 = layers.Dense(units)\n",
        "        self.W2 = layers.Dense(units)\n",
        "        self.V = layers.Dense(1)\n",
        "\n",
        "    def call(self, encoder_outputs, hidden):\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        score = self.V(tf.nn.tanh(self.W1(encoder_outputs) + self.W2(hidden_with_time_axis)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * encoder_outputs\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = layers.Embedding(spa_vocab_size, embedding_dim)\n",
        "        self.lstm = layers.LSTM(units, return_sequences=True, return_state=True)\n",
        "        self.fc = layers.Dense(spa_vocab_size)\n",
        "        self.attention = BahdanauAttention()\n",
        "\n",
        "    def call(self, x, hidden, cell, encoder_outputs):\n",
        "        context_vector, attention_weights = self.attention(encoder_outputs, hidden)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state_h, state_c = self.lstm(x, initial_state=[hidden, cell])\n",
        "        output = self.fc(output)\n",
        "        return output, state_h, state_c\n",
        "\n",
        "\n",
        "encoder = Encoder()\n",
        "decoder = Decoder()\n",
        "\n",
        "optimizer = keras.optimizers.Adam()\n",
        "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "\n",
        "epochs = 300\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_h, enc_c = encoder(eng_seq)\n",
        "\n",
        "        dec_input = tf.expand_dims(spa_seq[:, 0], 1)\n",
        "        dec_hidden, dec_cell = enc_h, enc_c\n",
        "\n",
        "        for t in range(1, spa_max_len):\n",
        "            predictions, dec_hidden, dec_cell = decoder(\n",
        "                dec_input, dec_hidden, dec_cell, enc_output\n",
        "            )\n",
        "            loss = loss_object(spa_seq[:, t], predictions[:, 0])\n",
        "            total_loss += loss\n",
        "            dec_input = tf.expand_dims(spa_seq[:, t], 1)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(total_loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss.numpy():.4f}\")\n",
        "\n",
        "\n",
        "def translate(sentence):\n",
        "    seq = eng_tokenizer.texts_to_sequences([sentence])\n",
        "    seq = keras.preprocessing.sequence.pad_sequences(seq, maxlen=eng_max_len, padding='post')\n",
        "\n",
        "    enc_output, enc_h, enc_c = encoder(seq)\n",
        "\n",
        "    dec_input = tf.expand_dims([spa_tokenizer.word_index['<start>']], 0)\n",
        "    dec_hidden, dec_cell = enc_h, enc_c\n",
        "\n",
        "    result = \"\"\n",
        "\n",
        "    for t in range(spa_max_len):\n",
        "        predictions, dec_hidden, dec_cell = decoder(\n",
        "            dec_input, dec_hidden, dec_cell, enc_output\n",
        "        )\n",
        "        predicted_id = tf.argmax(predictions[0][0]).numpy()\n",
        "\n",
        "        if spa_tokenizer.index_word.get(predicted_id) == \"<end>\":\n",
        "            break\n",
        "\n",
        "        result += spa_tokenizer.index_word.get(predicted_id, '') + \" \"\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result.strip()\n",
        "\n",
        "print(\"\\n===== Translation Results =====\")\n",
        "print(\"hello →\", translate(\"hello\"))\n",
        "print(\"thank you →\", translate(\"thank you\"))\n",
        "print(\"good morning →\", translate(\"good morning\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEv_0kOoaxsZ",
        "outputId": "19a8a6cf-0239-49c7-bb0a-393797597c41"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocabulary Size: 11\n",
            "Spanish Vocabulary Size: 11\n",
            "Epoch 0, Loss: 7.1950\n",
            "Epoch 50, Loss: 2.0641\n",
            "Epoch 100, Loss: 0.0602\n",
            "Epoch 150, Loss: 0.0170\n",
            "Epoch 200, Loss: 0.0089\n",
            "Epoch 250, Loss: 0.0057\n",
            "\n",
            "===== Translation Results =====\n",
            "hello → hola\n",
            "thank you → gracias\n",
            "good morning → buenos dias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Use the following short poetry dataset to simulate poem generation with a\n",
        "# pre-trained GPT model.\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "prompt = \"\"\"\n",
        "Roses are red, violets are blue,\n",
        "Sugar is sweet, and so are you.\n",
        "The moon glows bright in silent skies,\n",
        "\"\"\"\n",
        "\n",
        "poem = generator(prompt, max_length=80, num_return_sequences=1)\n",
        "print(poem[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711,
          "referenced_widgets": [
            "9a7cf818231d4760a416960617a722be",
            "68fdf88eaef34620981bf12098122a38",
            "02a9d98dc3de435cb8ee75d2e99cddac",
            "32a2cf6d1bbd474fbfec3ec784903b9e",
            "bde87639d31944f7aeb3163e799830c6",
            "3d818fa490a04892a10d95235c42d127",
            "e9cb5789a0db402cbc97a2c91d6e2e32",
            "ef72251026824aa9bd22b373286e827c",
            "04df395cb5e944ccbec0a16e322d68b1",
            "c083c6741a454e919cb841d05f74f763",
            "b28ae435300641638632303050a34295"
          ]
        },
        "id": "xHQXsvvTWW0i",
        "outputId": "62459b62-c030-4091-9b42-455e8c710f5e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a7cf818231d4760a416960617a722be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
            "Key                  | Status     |  | \n",
            "---------------------+------------+--+-\n",
            "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "Passing `generation_config` together with generation-related arguments=({'max_length', 'num_return_sequences'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Roses are red, violets are blue,\n",
            "Sugar is sweet, and so are you.\n",
            "The moon glows bright in silent skies,\n",
            "And when the sun sets down, the whole world will be happy,\n",
            "And the starry heavens will be filled with sunshine.\n",
            "Worth it?\n",
            "Why do you say it?\n",
            "Sugar is sweet, and so are you.\n",
            "The moon glows bright in silent skies,\n",
            "And when the sun sets down, the whole world will be happy,\n",
            "And the starry heavens will be filled with sunshine.\n",
            "Worth it? Why do you say it?\n",
            "Sugar is sweet, and so are you.\n",
            "The moon glows bright in silent skies,\n",
            "And when the sun sets down, the whole world will be happy,\n",
            "And the starry heavens will be filled with sunshine.\n",
            "Worth it? Why do you say it?\n",
            "Sugar is sweet, and so are you.\n",
            "The moon glows bright in silent skies,\n",
            "And when the sun sets down, the whole world will be happy,\n",
            "And the starry heavens will be filled with sunshine.\n",
            "Worth it? Why do you say it?\n",
            "Sugar is sweet, and so are you.\n",
            "The moon glows bright in silent skies,\n",
            "And when the sun sets down, the whole world will be happy,\n",
            "And the starry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10. Imagine you are building a creative writing assistant for a publishing company. The assistant should generate story plots and character descriptions using Generative AI. Describe how you would design the system, including model selection, training data, bias mitigation, and evaluation methods. Explain the real-world challenges you might face.\n",
        "\n",
        "Answer : If I were building a creative writing assistant for a publishing company, the goal would be to design a system capable of generating:\n",
        "\n",
        "Original story plots\n",
        "\n",
        "Detailed character descriptions\n",
        "\n",
        "Genre-specific narratives\n",
        "\n",
        "Creative story continuations\n",
        "\n",
        "The system must be creative, safe, unbiased, and commercially usable.\n",
        "\n",
        "1. System Architecture Design\n",
        "\n",
        "The system would consist of the following components:\n",
        "\n",
        "(A) User Input Interface\n",
        "\n",
        "Genre selection (Fantasy, Thriller, Romance, Sci-Fi, etc.)\n",
        "\n",
        "Tone selection (Dark, Humorous, Inspirational)\n",
        "\n",
        "Target audience (Children, YA, Adult)\n",
        "\n",
        "Optional keywords (e.g., “dragon”, “detective”, “space colony”)\n",
        "\n",
        "(B) Prompt Engineering Layer\n",
        "\n",
        "Converts user input into structured prompts.\n",
        "\n",
        "Example:\n",
        "\n",
        "“Generate a 3-paragraph fantasy plot with a strong female protagonist and a moral dilemma.”\n",
        "\n",
        "(C) Generative Model Layer\n",
        "\n",
        "Transformer-based Large Language Model (LLM)\n",
        "\n",
        "Fine-tuned for storytelling tasks\n",
        "\n",
        "(D) Post-processing & Filtering\n",
        "\n",
        "Toxicity filtering\n",
        "\n",
        "Bias detection\n",
        "\n",
        "Grammar correction\n",
        "\n",
        "Style consistency check\n",
        "\n",
        "(E) Human Review (Optional but Recommended)\n",
        "\n",
        "Editors validate creativity and originality.\n",
        "\n",
        "2. Model Selection\n",
        "Recommended Model Types:\n",
        "\n",
        "GPT-based Transformer Models\n",
        "\n",
        "Strong text generation capability\n",
        "\n",
        "Handles long context\n",
        "\n",
        "Fine-Tuned LLM\n",
        "\n",
        "Trained on storytelling datasets\n",
        "\n",
        "Improves narrative coherence\n",
        "\n",
        "Instruction-Tuned Models\n",
        "\n",
        "Better control over output style and format\n",
        "\n",
        "Why Transformer?\n",
        "\n",
        "Self-attention mechanism captures long-range dependencies.\n",
        "\n",
        "Generates coherent multi-paragraph text.\n",
        "\n",
        "Scalable and adaptable.\n",
        "\n",
        "3. Training Data Strategy\n",
        "Data Sources:\n",
        "\n",
        "Public domain novels (e.g., classic literature)\n",
        "\n",
        "Licensed publishing archives\n",
        "\n",
        "Screenplay datasets\n",
        "\n",
        "Character description databases\n",
        "\n",
        "Genre-specific corpora\n",
        "\n",
        "Data Preprocessing:\n",
        "\n",
        "Remove copyrighted non-licensed material\n",
        "\n",
        "Clean formatting\n",
        "\n",
        "Remove toxic/harmful content\n",
        "\n",
        "Tag genre, tone, and character attributes\n",
        "\n",
        "Data Annotation:\n",
        "\n",
        "Label by:\n",
        "\n",
        "Genre\n",
        "\n",
        "Emotional tone\n",
        "\n",
        "Character archetype\n",
        "\n",
        "Narrative structure\n",
        "\n",
        "4. Bias Mitigation Strategy\n",
        "\n",
        "Generative AI may inherit bias from training data. To reduce this:\n",
        "\n",
        "Methods:\n",
        "\n",
        "Dataset filtering\n",
        "\n",
        "Remove harmful stereotypes\n",
        "\n",
        "Balanced representation\n",
        "\n",
        "Include diverse authors, cultures, and perspectives\n",
        "\n",
        "Reinforcement Learning with Human Feedback (RLHF)\n",
        "\n",
        "Human reviewers rate fairness\n",
        "\n",
        "Bias Detection Tools\n",
        "\n",
        "Automatic toxicity and stereotype detection APIs\n",
        "\n",
        "Content moderation layer\n",
        "\n",
        "Blocks discriminatory or offensive outputs\n",
        "\n",
        "5. Evaluation Methods\n",
        "Automatic Metrics:\n",
        "\n",
        "Perplexity (fluency)\n",
        "\n",
        "BLEU / ROUGE (structure similarity)\n",
        "\n",
        "Diversity score (creativity measure)\n",
        "\n",
        "Human Evaluation:\n",
        "\n",
        "Creativity rating\n",
        "\n",
        "Emotional impact\n",
        "\n",
        "Originality\n",
        "\n",
        "Commercial suitability\n",
        "\n",
        "A/B Testing:\n",
        "\n",
        "Compare model versions\n",
        "\n",
        "Measure reader engagement\n",
        "\n",
        "6. Real-World Challenges\n",
        "\n",
        "Copyright & Ownership\n",
        "\n",
        "Who owns AI-generated content?\n",
        "\n",
        "Hallucination\n",
        "\n",
        "Model may generate inconsistent plot details\n",
        "\n",
        "Bias & Cultural Sensitivity\n",
        "\n",
        "Risk of stereotypes\n",
        "\n",
        "Ethical Misuse\n",
        "\n",
        "Plagiarism risk\n",
        "\n",
        "High Computational Cost\n",
        "\n",
        "Fine-tuning large models is expensive\n",
        "\n",
        "Controllability\n",
        "\n",
        "Hard to guarantee exact narrative structure\n",
        "\n",
        "Editorial Trust\n",
        "\n",
        "Publishers may resist AI adoption"
      ],
      "metadata": {
        "id": "3f47GT2MXVmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "prompt_plot = \"\"\"\n",
        "Generate a fantasy story plot about a young woman who discovers she can control time.\n",
        "Include conflict and a moral dilemma.\n",
        "\"\"\"\n",
        "\n",
        "plot = generator(prompt_plot, max_length=200, num_return_sequences=1)\n",
        "print(\"Generated Story Plot:\\n\")\n",
        "print(plot[0]['generated_text'])\n",
        "prompt_character = \"\"\"\n",
        "Create a detailed character description for a mysterious detective in a cyberpunk city.\n",
        "Include personality traits and backstory.\n",
        "\"\"\"\n",
        "\n",
        "character = generator(prompt_character, max_length=200, num_return_sequences=1)\n",
        "print(\"\\nGenerated Character Description:\\n\")\n",
        "print(character[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850,
          "referenced_widgets": [
            "30b02857e768405fa7804f1e60dfb792",
            "9c6e34d6898f43f89e6e94b1cf7d28d3",
            "d684b3568f4244e8b8a54a08a094e71c",
            "74aa0107822743e7bbf1f47b8c62a38d",
            "d0fff3eee8d943788f0a160574f72fdf",
            "78ebe790756b433d8dbf44b26fd4706d",
            "90ce0b02059541938375d948fd138d85",
            "e88f5389a9c94b0f9a029ae8a26fdb78",
            "75f2fee0f19741fe97110659c31b1d9e",
            "793fe369c83d4475af29c748f5f5714a",
            "69d8f80719b94b08994da83e4709ec41"
          ]
        },
        "id": "kfIJRN5TXqij",
        "outputId": "ecead09d-93f5-4503-a2a4-64df7f43e460"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30b02857e768405fa7804f1e60dfb792"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
            "Key                  | Status     |  | \n",
            "---------------------+------------+--+-\n",
            "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Story Plot:\n",
            "\n",
            "\n",
            "Generate a fantasy story plot about a young woman who discovers she can control time.\n",
            "Include conflict and a moral dilemma.\n",
            "Create a story in which the protagonist is the protagonist of a fantasy story.\n",
            "Create a story that tells a story about a character who is in love with the hero, and who has a romantic relationship with the hero.\n",
            "Include the heroine in the story.\n",
            "Include the heroine in the story.\n",
            "Add the heroes and villains in the story.\n",
            "The heroine can be a male or female.\n",
            "The hero is the hero with the greatest power and can be a powerful villain.\n",
            "The heroine can be a woman or a man.\n",
            "The hero cannot be killed or captured, for example, by a human.\n",
            "The hero can be a slave, a person, a person, a woman or a male slave.\n",
            "The hero can be a member of the military, the army or a person who is a warrior.\n",
            "The hero can be a monster, a god, a demon, a demon girl or a god of war, a demon woman or a god of war.\n",
            "The heroine can be a female or a male.\n",
            "The hero can be a man or a woman.\n",
            "The heroine can not be married.\n",
            "The heroine can be a man or a woman.\n",
            "The heroine can be an ally or a friend, a friend or a\n",
            "\n",
            "Generated Character Description:\n",
            "\n",
            "\n",
            "Create a detailed character description for a mysterious detective in a cyberpunk city.\n",
            "Include personality traits and backstory.\n",
            "Create a character sheet that describes the character's current lifestyle, actions, and experiences.\n",
            "Use this information to describe the character's personality, and to explore the character's past and present.\n",
            "Create a character sheet for the character's current job or role, or for a job in the criminal underworld.\n",
            "Add any other information you require, like the character's current social situation, family status, or family status.\n",
            "Create a character sheet for the character's current job or role, or for a job in the criminal underworld. Add any other information, like the character's current social situation, family status, or family status. Create a character sheet for the character's current job or role, or for a job in the criminal underworld. Add any other information, like the character's current social situation, family status, or family status. Create a character sheet for the character's current job or role, or for a job in the criminal underworld. Add any other information, like the character's current social situation, family status, or family status.\n",
            "Add any other information, like the character's current social situation, family status, or family status. Add any other information, like the character's current social situation, family status, or family status. Add any other information\n"
          ]
        }
      ]
    }
  ]
}